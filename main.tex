\documentclass[11pt]{beamer}

\usetheme{epfl}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows.meta}
\usetikzlibrary{calc,patterns,angles,quotes, fadings}
\usetikzlibrary{decorations,fit}

\usepackage{media9}
\usepackage{amsmath}
\usepackage[thicklines]{cancel}
\usepackage{svg}
\newcommand{\citeapa}[1]{ {\tiny#1\par} }
%\usepackage{utf8}
%\usepackage{fontspec}

% primary colors
\definecolor{tumblue}{RGB}{0,101,189} % Pantone 300
\definecolor{tumorange}{RGB}{227,114,34} % Pantone 158 - Orange
\setbeamercovered{transparent}

\begin{document}
	
	\title{Data-Driven Vegetation Modeling and Tackling Representation Shift}
	\subtitle{with Few-Shot Meta-Learning Methods}
	\author{Marc Russwurm}
	\labname{EPFL-ECEO}
	\date{\today}
	
	\logofilename{epfl/epfl_red.png}
	\titlepageimage{epfl/lacleman}
	\event{Deep Learning for Environment}
	\presentation{Representation Shift and Few-Shot Meta-Learning Methods}
	\speaker{Marc Russwurm}
	
	\begin{frame}[plain]
		\maketitle
	\end{frame}

	\begin{frame}{Structure}
		
		\begin{enumerate}
			\item Data-Driven Vegetation Modeling and 
			\begin{itemize}
				\item why is it meaningful
				\item data-driven verus model-driven approaches
			\end{itemize}
			\item<2-> Tackling Representation Shift 
			\begin{itemize}
				\item when does it arise?
				\item regional representations
			\end{itemize}
			\item<3> with Few-Shot Meta-Learning Methods
			\begin{itemize}
				\item seeing our remote sensing data as a dataset-of-datasets
				\item Model-agnostic meta-learning as one algorithm to tackle it.
			\end{itemize}
		\end{enumerate}
		
	\end{frame}

	\begin{frame}{Vegetation Modeling}
		\begin{tikzpicture}
			\node[fill=EPFLred, inner sep=1em, text=white, font=\bfseries]{Vegetation Modeling};
		\end{tikzpicture}
	\end{frame}


	\begin{frame}
		\frametitle{Why do we want to classify vegetation?}
		\begin{columns}
			\column{.5\textwidth}
			{\scriptsize
			\begin{description}
				\item[Zero Hunger] estimating the \textbf{crop yield} \& predict \textbf{food prices}, shortages.
				\item<2->[Climate Action] by monitoring \textbf{vegetation changes} over time (desertification).
				\item<3->[Life on Land] human life depends on \textbf{agriculture}, animal life on \textbf{biotopes}.
				%				\item Precision agriculture: sd
			\end{description}
			}
			\column{.5\textwidth}
			\includegraphics[width=\textwidth]{images/sdg_goals.pdf}
		\end{columns}
		
	\end{frame}

	\begin{frame}{Remote Sensing Data in Abundance}
		\begin{columns}[t]
			\column{.33\textwidth}
				MODIS
				\includemedia[
				width=\textwidth,
				activate=pageopen,
				addresource=mp4/apenne_modis_ndvi.mp4,
				flashvars={source=mp4/apenne_modis_ndvi.mp4&loop=true&
					autoPlay=true}
				]{\includegraphics[width=\textwidth]{mp4/apenne_modis_ndvi.png}}{mp4/apenne_modis_ndvi.mp4}
				{\tiny visualization \href{https://github.com/aaronpenne}{Aaron Penne Github}}
			
%				\vfill
%				\scriptsize daily, global scale, 36 bands (1km by 1km pixels)
			\column{.33\textwidth}
				Sentinel/Landsat
				\includemedia[
				width=\textwidth,
				activate=pageopen,
				addresource=mp4/sentinel2.mp4,
				flashvars={source=mp4/sentinel2.mp4&loop=true&
					autoPlay=true}
				]{\includegraphics[width=\textwidth]{mp4/sentinel2.png}}{mp4/sentinel2.mp4}
				{\tiny visualization ESA}
%				\vfill
%				\scriptsize 2-5 days, regional scale, 13 bands (10m by 10m pixels)
			\column{.33\textwidth}
				PlanetScope
				\includemedia[
				width=\textwidth,
				activate=pageopen,
				addresource=mp4/planet_denethor.mp4,
				flashvars={source=mp4/planet_denethor.mp4&loop=true&
					autoPlay=true}
				]{\includegraphics[width=\textwidth]{mp4/planet_denethor.png}}{mp4/planet_denethor.mp4}
				{\tiny DENETHOR dataset /\\ Kondmann et al., (2021)\par}
%				\vfill
%				\scriptsize daily, local scale, 4 bands (3m by 3m pixels)
		\end{columns}
	
		\vspace{1em}
		\small
	
		\begin{columns}
			\column{.33\textwidth}		
			\begin{itemize}
				\item daily/half-monthly
				\item 250m by 250m px
				\item 39 bands
			\end{itemize}	
			\column{.33\textwidth}
			\begin{itemize}
				\item 2-5 days
				\item 10m by 10m px
				\item 13 bands
			\end{itemize}
			\column{.33\textwidth}
			\begin{itemize}
				\item daily
				\item 3m by 3m pixels
				\item 4 bands
			\end{itemize}

	\end{columns}
	
		\vspace{3mm}
		\citeapa{Kondmann, et al., "DENETHOR: The DynamicEarthNET dataset for Harmonized, inter-Operable, analysis-Ready, daily crop monitoring from space.". NeurIPS Data Track (2021).}
	\end{frame}

	
	\begin{frame}
		\frametitle{Photosynthesis: Light $\rightarrow$ Life}
		%		\framesubtitle{Vegetation}
		
		\begin{columns}[b]
			
			\column{.5\textwidth}
			\includegraphics[width=\textwidth]{images/leaf_smallannot.pdf}
%			common remote sensing feature: $\text{NDVI} = \frac{\text{NIR} - \text{RED}}{\text{NIR} + \text{RED}}$ \\
			{\tiny Image adapted from wikipedia cc}
			%
			\column{.5\textwidth}
			\includegraphics[width=\textwidth]{images/reflectance_vegetation}
			\tiny Image from Roman et al., (2016)
		\end{columns}
		
		\normalsize
		
		\vspace{1em}
		Normalized Difference Vegetation Index.
		\vspace{1em}
		\begin{align}
			\text{NDVI}(\text{red}, \text{nir}) = \frac{\text{nir}-\text{red}}{\text{nir}+\text{red}}
		\end{align}
		
		\vfill
		
		\citeapa{Roman, A., \& Ursu, T. (2016). Multispectral satellite imagery and airborne laser scanning techniques for the detection of archaeological vegetation marks. Landsc. Archaeol. North. Front. Rom. Emp. Porolissum, 141-152.}
		
	\end{frame}

	\begin{frame}<presentation:1>
		\frametitle{Satellite Time Series}
		
		\begin{tikzpicture}[node distance=0em]
		\node(a) at (0,0){\includegraphics[width=10cm]{images/denethor/43647_Meadows_images}};
		\node(b)[below=0em of a]{\includegraphics[width=10cm]{images/denethor/82235_Wheat_images}};
		\visible<1>{
			\node(c)[below=1cm of b, xshift=-4em]{\includegraphics[width=.8\textwidth]{images/denethor/plots_2018.pdf}};
		}
	
		\visible<2>{
			\node(c)[below=1cm of b, xshift=-4em]{\includegraphics[width=.8\textwidth]{images/denethor/plotss2.pdf}};
		}
		\node(d)[right=-2em of c, yshift=1em]{\includegraphics[width=3cm]{images/denethor/map}};
		
		%\node[below=of c]{\includegraphics[width=.9\textwidth]{images/plotss2}};
		
		\definecolor{mpldefaultblue}{HTML}{1f77b4}
		%\coodinate (l1) at ($ (c) + (1,1) $);
%		\draw[Circle - , color=mpldefaultblue] ($ (c) + (-.45,1.6) $) -- ($ (b) + (-3.8,-.8) $); % requires \usetikzlibrary{arrows.meta} and calc
%		\draw[Circle - , color=mpldefaultblue] ($ (c) + (.78,1.6) $) -- ($ (b) + (-1.4,-.8) $);
%		\draw[Circle - , color=mpldefaultblue] ($ (c) + (1.38,1.6) $) -- ($ (b) + (1.6,-.8) $);
%		\draw[Circle - , color=mpldefaultblue] ($ (c) + (2.46,1.6) $) -- ($ (b) + (5,-.8) $);
		
		%\node[circle, draw] at (l1) {+};
		
		\end{tikzpicture}
		
		\citeapa{Kondmann, et al., "DENETHOR: The DynamicEarthNET dataset for Harmonized, inter-Operable, analysis-Ready, daily crop monitoring from space.". NeurIPS Data Track (2021).}
	\end{frame}

	\begin{frame}{Model-Driven Methods}
		\begin{columns}
			\column{.5\textwidth}
				\resizebox{\textwidth}{!}{
					\begin{tikzpicture}
					\node[label={above:\scriptsize growth stages of corn Mimic et al 2020}, inner sep=0](a){\includegraphics[width=6cm]{images/example/mimicetal2020_phenology_maize_}};
					
					\node[below=1em of a] (b){\includegraphics{images/example/NDVI}};
					
					\coordinate(left)  at ($(a.south west)+(-4.3em,0)$);
					\coordinate(right) at ($(a.south east)+(1.1em,0)$);
					
					\coordinate(veannot) at ($(left)!0.22!(right)$){};
					\coordinate(vtwoannot) at ($(left)!0.275!(right)$){};
					\coordinate(vfiveannot) at ($(left)!0.34!(right)$){};
					\coordinate(vtenannot) at ($(left)!0.42!(right)$){};
					\coordinate(btnnot) at ($(left)!0.5!(right)$){};
					\coordinate(rtwoannot) at ($(left)!0.6!(right)$){};
					\coordinate(rthreeannot) at ($(left)!0.7!(right)$){};
					\coordinate(rfiveannot) at ($(left)!0.8!(right)$){};
					\coordinate(harvestannot) at ($(left)!0.9!(right)$){};
					
					\coordinate(ve) at      ($ (b.center)+(-4em  ,1em) $);
					\coordinate(vtwo) at    ($ (b.center)+(-3em  ,1em) $);
					\coordinate(vfive) at   ($ (b.center)+(-2.5em,3em) $);
					\coordinate(vten) at    ($ (b.center)+(-2em  ,4em) $);
					\coordinate(bt) at      ($ (b.center)+(-1.5em,4.5em) $);
					\coordinate(rtwo) at    ($ (b.center)+(-1em  ,4.5em) $);
					\coordinate(rthree) at  ($ (b.center)+(-0em  ,2em) $);
					\coordinate(rfive) at   ($ (b.center)+(1.5em ,0.5em) $);
					\coordinate(harvest) at ($ (b.center)+(3em   ,0.5em) $);
					
					\draw[Circle-,tumorange](veannot)      -- (ve);
					\draw[Circle-,tumorange](vtwoannot)    -- (vtwo);
					\draw[Circle-,tumorange](vfiveannot)   -- (vfive);
					\draw[Circle-,tumorange](vtenannot)    -- (vten);
					\draw[Circle-,tumorange](btnnot)       -- (bt);
					\draw[Circle-,tumorange](rtwoannot)    -- (rtwo);
					\draw[Circle-,tumorange](rthreeannot)  -- (rthree);
					\draw[Circle-,tumorange](rfiveannot)   -- (rfive);
					\draw[Circle-,tumorange](harvestannot) -- (harvest);
					\end{tikzpicture}
				}
			\column{.5\textwidth}
				\includegraphics[width=\textwidth]{images/example/jointplot}
			
		\end{columns}
	
		\begin{tikzpicture}
			\tikzstyle{process} = [draw, rounded corners]
			\node(x){input $\mathbf{X}$};
			\node[process, right=of x](f){feature definition};
			\node[process, right=of f](c){classification};
			\node[right=of c](y){label $y$};
			\draw[-Stealth](x) -- (f);
			\draw[-Stealth](f) -- (c);
			\draw[-Stealth](c) -- (y);
			\node[below=0cm of f, font=\scriptsize](a){mean NDVI};
			\node[below=0cm of a, font=\scriptsize](b){max NDVI after Sept};
			
			\node[below=0cm of c, font=\scriptsize](a){e.g., SVM, RF};
		\end{tikzpicture}
		
	\end{frame}

	
	\newcommand{\nn}{
		\tikzstyle{proba} = [circle, draw=white, inner sep=2.5pt, fill=rouge]
		\begin{tikzpicture}[scale=0.5, rotate=0, baseline=-.25em, minimum width=0cm, minimum height=0cm]
		\node[proba](a0) at (0,-1){};
		\node[proba](a1) at (0,0){};
		\node[proba](a2) at (0,1){};
		
		\node[proba](b0) at (1,-0.5){};
		\node[proba](b1) at (1,0.5){};
		
		\draw[-] (a0) -- (b0);
		\draw[-] (a1) -- (b0);
		\draw[-] (a2) -- (b0);
		
		\draw[-] (a0) -- (b1);
		\draw[-] (a1) -- (b1);
		\draw[-] (a2) -- (b1);
		
		%	\node[fit=(a0)(a2)(b1)](node name){};
		
		\end{tikzpicture}
	}

	\newcommand{\featurelearningtikz}{
		\begin{tikzpicture}
			\tikzstyle{process} = [draw, rounded corners]
			\node(x){input $\mathbf{X}$};
			\node[process, right=of x](f){feature learning $f_\mathbf{\theta}$ \nn};
			\node[right=of f](y){label $y$};
			\draw[-Stealth](x) -- (f);
			\draw[-Stealth](f) -- (y);
		\end{tikzpicture}
	}

	\begin{frame}[t]{Data-Driven Methods}
		
		
		\centering
		\featurelearningtikz
		
		\vspace{1em}
		
		\raggedright
		
		\resizebox{\textwidth}{!}{
			\input{tikzfigures/NN-transformations.tikz}
		}
	\end{frame}

	\begin{frame}[t]{Data-Driven Methods}
		\centering
		\featurelearningtikz
		
		\hspace{-1em} Search for suitable data-specific architecture
			
			\begin{columns}[t]
				
				\column{.5\textwidth}
				
				In general
					\begin{itemize}
						\item CNNs for images (e.g. ResNet50)
						\item Transformers/RNNs for language (e.g. Bert)
					\end{itemize}
				\column{.5\textwidth}
				
				For crop type mapping
				\begin{itemize}
					\item CNNs 
					\item RNNs 
					\item Transformers
				\end{itemize}
			\end{columns}
		
		\vspace{2em}
			\centering
			{Data-driven deep learning models are very plastic and (blindly) learn to approximate target data.}\\
			
			\vspace{1em}
			We need to think carefully how we sample and select data. 
			
	\end{frame}

	\newcommand{\iidsim}{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily i.i.d}}}{\sim}}}
	\begin{frame}[t]{Data Generation Model}
		\centering
%		
%		\featurelearningtikz
%		
%		\vspace{1em}
		
		\begin{tikzpicture}
			\node[label={[name=domainlabel]domain}](domain){$\mathcal{D} = \{\mathcal{X}, P(\mathcal{X})\}$};
			\node[label={[name=tasklabel]task}, right=of domain](task){$\mathcal{T} = \{\mathcal{Y}, f(\cdot)\}$};
			
			\node[below=of domain](xsample){$\mathbf{X} \sim P(\mathcal{X})$};
			
			\node[below=of task](ysample){$y = f(\mathbf{X})$};
			
			\draw[-Stealth] (domain) -- (xsample);
			\draw[-Stealth] (xsample) -- node[midway, above, font=\tiny, name=label]{predictive function f} (ysample);
			
			\node[fit=(domain)(xsample)(task)(ysample)(domainlabel)(tasklabel), draw, rounded corners, label={[name=dgm, font=\small]below:(world) data generation model $P(\mathcal{X} \times \mathcal{Y})$}](box) {};
%			\node[right=of box, font]{Data Generation Model};

			
			\node[below=4em of label](datasampling){Dataset: $\{(\mathbf{X}_i,y_i)\}_{i=1}^N \iidsim P(\mathcal{X} \times \mathcal{Y})$};
			
			\node[below=1em of datasampling](model){\featurelearningtikz};
			
		\end{tikzpicture}
		\vfill\tiny
		\citeapa{Pan, S. J., \& Yang, Q. (2009). A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10), 1345-1359.}
		\citeapa{Qiang Yang; Yu Zhang, Wenyuan Dai; Sinno Jialin Pan (Editors) (2020). Transfer learning}
		\citeapa{Shai Ben-David and Shai Shalev-Shwartz (2014). Understanding Machine Learning: From Theory to Algorithms}
	\end{frame}

	
	\begin{frame}
		\frametitle{I.I.D assumption}
		
		we use a dataset $\{(\mathbf{X}_i,y_i)\}_{i=1}^N \iidsim P(\mathcal{X} \times \mathcal{Y})$ of \textbf{independently} and and \textbf{identically} distributed (i.i.d) samples.
		\vspace{1em}
		
		\pause
		we need \textbf{independence} for the factorization in max likelihood.
		\begin{align*}
		\theta^\ast &= \arg\max_\theta \prod_{i} p(y_i \vert \mathbf{X}_i)  \\
		&= \arg\max_\theta \sum_{i} -y_i \log f_\theta(\mathbf{X}_i)
		\end{align*}
		
		\pause
		we assume an \textbf{identical distribution} $P(\mathcal{X} \times \mathcal{Y})$ throughout the entire data generation { \scriptsize (and particularly between training and test time) }
		\begin{align}
		X_i,y_i \iidsim P(\mathcal{X} \times \mathcal{Y})
		\end{align}
		which implies identical domains $\mathcal{D} = \{\mathcal{X}, P(\mathcal{X})\}$ and tasks $\mathcal{T} = \{\mathcal{Y}, f(\cdot)\}$ during training and test time.
		
	\end{frame}

	\begin{frame}{In- and Out-of-Dist. Generalization}
		
		In Machine Learning, the I.I.D assumption is usually enforced via random splitting a single dataset $D$ into training and testing partitions
		
		\vspace{2em}
		
		\begin{tikzpicture}
			\node(ds) at (0,0){$D \iidsim P(\mathcal{X} \times \mathcal{Y})$};
			\node at (-1,-1)(dstrain){$D_\text{train}$};
			\node at (1,-1)(dstest){$D_\text{test}$};
			
			\draw[-Stealth](ds) --node[midway, left=1em, font=\tiny, name=labelsplit]{random split} (dstrain);
			\draw[-Stealth](ds) -- (dstest);
			
			\node[draw, rounded corners, fit=(ds)(dstrain)(dstest)(labelsplit), label={above: I.I.D Machine Learning}]{};
			
			\node[below=of dstrain, label={[font=\tiny]left:train model}] (weights) {$f_\theta$};
			\draw[-Stealth] (dstrain) -- (weights);
			
			\node[below=of dstest] (testloss) {$\mathcal{L}_\text{in}$};
			\draw[-Stealth] (dstest) -- (testloss);
			\draw[-Stealth] (weights) -- node[midway,above,font=\tiny]{compare with $f(\cdot)$} (testloss);
			
			\node[below=0em of testloss, text width=2cm, font=\scriptsize](tllabel){"in-domain" or "in-distribution" generalization};
			
			\visible<2->{
				\node[right=5em of ds](Q){$\mathbf{X},y \sim Q(\mathcal{X} \times \mathcal{Y})$};
				
				\node[below=0em of Q](domain){$\mathcal{D}_Q = \{\mathcal{X}, P_Q(\mathcal{X})\}$};
				\node[below=-.5em of domain](task){$\mathcal{T}_Q = \{\mathcal{Y}, f_Q(\cdot)\}$};
				
				\node[rounded corners, fit=(Q)(domain)(task), label={above: Data of another distribution\phantom{g}}]{};
				
				\node[right=8em of testloss] (oodloss) {$\mathcal{L}_\text{out}$};
				\node[below=0em of oodloss, text width=3cm, font=\scriptsize](oodlabel){"out-of-domain" or \\ "out-of-distribution" \\ generalization};
				
				\draw[-Stealth] (testloss) -- node[midway,above,font=\tiny]{compare with $f_Q(\cdot)$} (oodloss);
				\draw[-Stealth] (task) -- (oodloss);
			}
		\end{tikzpicture}
	\end{frame}

	\begin{frame}{Measuring OOD Generalization}
		\begin{tikzpicture}
		\node(ds) at (0,0){$D \iidsim P(\mathcal{X} \times \mathcal{Y})$};
		\node at (-1,-1)(dstrain){$D_\text{train}$};
		\node at (1,-1)(dstest){$D_\text{test}$};
		
		\draw[-Stealth](ds) --node[midway, left=1em, font=\tiny, name=labelsplit]{random split} (dstrain);
		\draw[-Stealth](ds) -- (dstest);
		
		\node[draw, rounded corners, fit=(ds)(dstrain)(dstest)(labelsplit), label={above: Source Task/Domain\phantom{g}}]{};
		
		\node[below=of dstrain, label={[font=\tiny]left:train model}] (weights) {$f_\theta$};
		\draw[-Stealth] (dstrain) -- (weights);
		
		\node[below=of dstest] (testloss) {$\mathcal{L}_\text{in}$};
		\draw[-Stealth] (dstest) -- (testloss);
		\draw[-Stealth] (weights) -- node[midway,above,font=\tiny]{compare with $f(\cdot)$} (testloss);
		
		\node[below=0em of testloss, text width=2cm, font=\scriptsize](tllabel){"in-domain" or "in-distribution" generalization};
		
		\begin{scope}[xshift=5cm]
			\node(Q) at (0,0){$D \iidsim Q(\mathcal{X} \times \mathcal{Y})$};
			\node at (-1,-1)(dsqtrain){$D_\text{train}$};
			\node at (1,-1)(dsqtest){$D_\text{test}$};
		\end{scope}
		
		\draw[-Stealth](Q) --node[midway, left=0.5em, font=\tiny, name=labelqsplit]{random split} (dsqtrain);
		\draw[-Stealth](Q) -- (dsqtest);
		
		\node[draw, rounded corners, fit=(Q)(dsqtrain)(dsqtest)(labelqsplit), label={above: Target Task/Domain}]{};
		
		\node[below=of dsqtest] (oodloss) {$\mathcal{L}_\text{out}$};
		\node[below=0em of oodloss, text width=3cm, font=\scriptsize](oodlabel){"out-of-domain" or \\ "out-of-distribution" \\ generalization};
		
		\draw[-Stealth] (testloss) -- node[midway,above,font=\tiny]{compare with $f_Q(\cdot)$} (oodloss);
		\draw[-Stealth] (dsqtest) -- (oodloss);
		
		\end{tikzpicture}
	\end{frame}

	\begin{frame}{Train on Each Dataset Separately}
		
	\end{frame}

	\begin{frame}{Global Distributions change with $\lambda, \varphi, t$}
		Dominant crop type (by color) in France: $P(y)$
		\includegraphics[width=\textwidth]{images/france_crop_distribution}
	\end{frame}


	\begin{frame}{Toy Example: "wheat" vs "other cereals"}
		\includegraphics[width=.49\textwidth]{images/francecrops/frf1.png}
		\includegraphics[width=.49\textwidth]{images/francecrops/fri1.png}
		
		\vspace{1em}
		binary "wheat" vs "other cereals" classification
	\end{frame}



	\begin{frame}{Representation Shift}
		\includegraphics[width=\textwidth]{images/countries_globe}
	\end{frame}
	
	\begin{frame}{Domain and Task}
		\begin{tikzpicture}
		\node[label=domain](domain){$\mathcal{D} = \{\mathcal{X}, P(\mathcal{X})\}$};
		\node[label=task, right=of domain](task){$\mathcal{T} = \{\mathcal{Y}, f(\cdot)\}$};
		\end{tikzpicture}
		\vfill\tiny
		\citeapa{Pan, S. J., \& Yang, Q. (2009). A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10), 1345-1359.}
		¨
	\end{frame}

	\begin{frame}{Shift in Remote Sensing Data}
		\includegraphics[width=.9\textwidth]{images/Sen12ms_distribution_shift}
		
		\citeapa{Rußwurm, M., Wang, S., Korner, M., \& Lobell, D. (2020). Meta-learning for few-shot land cover classification. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition workshops (pp. 200-201).}
	\end{frame}
	
	
	\begin{frame}{A Frame Title}
		
		\begin{columns}
			\column{.5\textwidth}
			
			
			This is the frame content
			\begin{itemize}
				\item with some itemize objects
				\item and another one
			\end{itemize}
			
			Enumerates should work too
			\begin{enumerate}
				\item one
				\item two
			\end{enumerate}
		
			\column{.5\textwidth}
			
			A second column with other content, maybe more text, or something \emph{emphasized}. For more, highlight in \emphred{red} or \emphblue{blue}.
			
			
			
		\end{columns}
		
	\end{frame}

		
		\begin{frame}<presentation:9,13,14,15>%1-9,
			\frametitle{Deep Neural Networks}
			\centering
			
			\input{tikzfigures/neuralnetwork.tikz}
	\end{frame}

	\begin{frame}{Colors}
		\centering
		\begin{tikzpicture}[xscale=2.6, yscale=-2]
		\tikzstyle{colorbox} = [minimum width=1.8cm, minimum height=1.8cm, text width=1.5cm, font=\scriptsize]
		
		\node(rouge) at (0,0)[fill=rouge,text=white, colorbox]{rouge};
		\node(leman) at (0,1)[fill=leman,text=white, colorbox]{leman};
		\node(grosseille) at (0,2)[fill=grosseille,text=white, colorbox]{grosseille};
		\node(canard) at (0,3)[fill=canard,text=white, colorbox]{canard};
		\node(montrose) at (1,0)[fill=montrose,text=white, colorbox]{montrose};
		\node(perle) at (1,1)[fill=perle,text=white, colorbox]{perle};
		\node(vertedeau) at (1,2)[fill=vertedeau,text=white, colorbox]{vertedeau};
		\node(rose) at (1,3)[fill=rose,text=white, colorbox]{rose};
		\node(acier) at (2,0)[fill=acier,text=white, colorbox]{acier};
		\node(soufre) at (2,1)[fill=soufre,text=white, colorbox]{soufre};
		\node(carotte) at (2,2)[fill=carotte,text=white, colorbox]{carotte};
		\node(zinzolin) at (2,3)[fill=zinzolin,text=white, colorbox]{zinzolin};
		\node(chartreuse) at (3,0)[fill=chartreuse,text=white, colorbox]{chartreuse};
		\node(marron) at (3,1)[fill=marron,text=white, colorbox]{marron};
		\node(ardoise) at (3,2)[fill=ardoise,text=white, colorbox]{ardoise};
		\node(taupe) at (3,3)[fill=taupe,text=white, colorbox]{taupe};
		
		\end{tikzpicture}
		
		
\end{frame}

\end{document}